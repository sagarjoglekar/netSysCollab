\section{Introduction}

Deep neural nets are progressing at an amazing pace over the past decade. The community as a whole has been breaking new ceilings 
when it comes to classification and inference records for specific tasks like object detection, scene detection, language modeling etc.
But the internal workings and the internal process of neural nets before coming to a particular decision, more or less still remains a mystery.
Neural nets have more or less remained a black box for its users. Explain-ability and understanding the deep reasoning behind decisions is 
one of the most researched problems in the machine learning community.
\par
The problem of explain-ability becomes even more abstract and obscured, when we are dealing with tasks that handle meta, and abstract quantities like
sentiment, affects and aesthetics. Despite the black box like nature, deep neural networks have done remarkable strides in understanding creativity \cite{redi20146}, 
memorability \cite{Isola2011} or beauty \cite{schifanella2015image}. 
These works explore perceptual qualities of media objects using deep learning, and treat the explainability of the models using round about methods like perturbation 
of input and understanding correlation of several governing variables with decisions of the network etc. These methods are perfectly valid and do give some interesting insights into the decision influencing factors for the models, however still fail to explain the decision making process of the model itself.  
\par 
This paper builds on top of several works done before, in the areas of explainability of neural nets and understanding affective dimensions as listed before takes a step
towards extending these to the realm of urban emotions and aesthetics. More so we strive to propose a generalizable pipeline for analyzing geo-referenced
images.